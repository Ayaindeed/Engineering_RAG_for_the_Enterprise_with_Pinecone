{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02616ea-8dbc-46d8-868c-92668e7f2533",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Engineering Retrieval Augmented Generation Applications for the Enterprise with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bea0e3-738d-46ec-a231-18d3ee5882a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20962444-cc78-4bd1-a50a-d3f427b65909",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e8623-8413-46f7-816e-d6d3ce551d03",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 2736,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1722352622681,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install pandas marko openai pyarrow fastparquet pinecone-client ragas torch \"rerankers[flashrank]\"",
    "outputsMetadata": {
     "0": {
      "height": 561,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/hp/Desktop/Engineering_Retrieval_Augmented_Generation_Applications_for_the_Enterprise_with_Pinecone/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install pandas marko openai pyarrow fastparquet pinecone-client ragas torch \"rerankers[flashrank]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10aae01-9245-4529-a7a7-e0f6113e2a12",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 707,
    "lastExecutedAt": 1722352677369,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\nimport pandas as pd\nfrom IPython.display import Markdown, display",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18c3b7-727c-471b-8a9f-4d4c95e4f35f",
   "metadata": {},
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5aefb-b615-4fc8-a230-93ec8ba3a572",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 7824,
    "lastExecutedAt": 1722352696230,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import getpass\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# API keys are now loaded from .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] is automatically set\n",
    "# os.environ[\"PINECONE_API_KEY\"] is automatically set\n",
    "\n",
    "print(\"✅ API keys loaded from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b515b74-502b-4df6-ad5a-e63f5040faa1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3cb1d3-521b-436d-aabe-6b4bc5e9e258",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 218,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1722352721540,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_parquet(\"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\")\ndata.head()",
    "outputsMetadata": {
     "0": {
      "height": 6616,
      "type": "dataFrame"
     }
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "id": [
          "728aeea1-1dcf-5d0a-91f2-ecccd4dd4272",
          "2f19f269-171f-5556-93f3-a2d7eabbe50f",
          "b2a71cb3-5148-5090-86d5-7f4156edd7cf",
          "1dafe68a-2e78-57f7-a97a-93e043462196",
          "8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd"
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ],
         "metadata": [
          {
           "created_at": "2023_10_25",
           "title": "scaling-indexes"
          },
          {
           "created_at": "2023_10_25",
           "title": "organizations"
          },
          {
           "created_at": "2023_10_25",
           "title": "datasets"
          },
          {
           "created_at": "2023_10_25",
           "title": "architecture"
          },
          {
           "created_at": "2023_10_25",
           "title": "moving-to-production"
          }
         ],
         "source": [
          "https://docs.pinecone.io/docs/scaling-indexes",
          "https://docs.pinecone.io/docs/organizations",
          "https://docs.pinecone.io/docs/datasets",
          "https://docs.pinecone.io/docs/architecture",
          "https://docs.pinecone.io/docs/moving-to-production"
         ],
         "text": [
          "# Scale indexes\n\n[Suggest Edits](/edit/scaling-indexes)In this topic, we explain how you can scale your indexes horizontally and vertically.\n\n\nProjects in the `gcp-starter` environment do not support the features referred to here, including pods, replicas, and collections.\n\n\n## Vertical vs. horizontal scaling\n\n\nIf you need to scale your environment to accommodate more vectors, you can modify your existing index to scale it vertically or create a new index and scale horizontally. This article will describe both methods and how to scale your index effectively. \n\n\n## Vertical scaling\n\n\nScaling vertically is fast and involves no downtime. This is a good choice when you can't pause upserts and must continue serving traffic. It also allows you to double your capacity instantly. However, there are some factors to consider.\n\n\nBy [changing the pod size](manage-indexes#changing-pod-sizes), you can scale to x2, x4, and x8 pod sizes, which means you are doubling your capacity at each step. Moving up to a new capacity will effectively double the number of pods used at each step. If you need to scale by smaller increments, then consider horizontal scaling. \n\n\nThe number of base pods you specify when you initially create the index is static and cannot be changed. For example, if you start with 10 pods of `p1.x1` and vertically scale to `p1.x2`, this equates to 20 pods worth of usage. Neither can you change pod types with vertical scaling. If you want to change your pod type while scaling, then horizontal scaling is the better option. \n\n\nYou can only scale index sizes up and cannot scale them back down.\n\n\nSee our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\n\n\n## Horizontal scaling\n\n\nThere are two approaches to horizontal scaling in Pinecone: adding pods and adding replicas. Adding pods increases all resources but requires a pause in upserts; adding replicas only increases throughput and requires no pause in upserts.\n\n\n### Adding pods\n\n\nAdding pods to an index increases all resources, including available capacity. Adding pods to an existing index is possible using our <collections> feature. A collection is an immutable snapshot of your index in time: a collection stores the data but not the original index definition.\n\n\nWhen you [create an index from a collection](manage-indexes#create-an-index-from-a-collection), you define the new index configuration. This allows you to scale the base pod count horizontally without scaling vertically. The main advantage of this approach is that you can scale incrementally instead of doubling capacity as with vertical scaling. Also, you can redefine pod types if you are experimenting or if you need to use a different pod type, such asperformance-optimized pods or storage-optimized pods. Another advantage of this method is that you can change your [metadata configuration](manage-indexes#selective-metadata-indexing) to redefine metadata fields as indexed or stored-only. This is important when [tuning your index](performance-tuning) for the best throughput. \n\n\nHere are the general steps to make a copy of your index and create a new index while changing the pod type, pod count, metadata configuration, replicas, and all typical parameters when creating a new collection: \n\n\n1. Pause upserts.\n2. Create a collection from the current index.\n3. Create an index from the collection with new parameters.\n4. Continue upserts to the newly created index. Note: the URL has likely changed.\n5. Delete the old index if desired.\n\n\n### Adding replicas\n\n\nEach replica duplicates the resources and data in an index. This means that adding additional replicas increases the throughput of the index but not its capacity. However, adding replicas does not require downtime.\n\n\nThroughput in terms of queries per second (QPS) scales linearly with the number of replicas per index.\n\n\nTo add replicas, use the `configure_index` operation to [increase the number of replicas for your index](manage-indexes#replicas).\n\n\n## Next steps\n\n\n* See our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\n* Learn more about <collections>.\nUpdated 29 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Vertical vs. horizontal scaling](#vertical-vs-horizontal-scaling)\n\t+ [Vertical scaling](#vertical-scaling)\n\t+ [Horizontal scaling](#horizontal-scaling)\n\t\t- [Adding pods](#adding-pods)\n\t\t- [Adding replicas](#adding-replicas)\n\t+ [Next steps](#next-steps)\n",
          "# Understanding organizations\n\n[Suggest Edits](/edit/organizations)## Overview\n\n\nA Pinecone organization is a set of <projects> that use the same billing. Organizations allow one or more users to control billing and project permissions for all of the projects belonging to the organization. Each project belongs to an organization. \n\n\nFor a guide to adding users to an organization, see [Add users to a project or organization](add-users-to-projects-and-organizations/).\n\n\n## Projects in an organization\n\n\nEach organization contains one or more projects that share the same organization owners and billing settings. Each project belongs to exactly one organization. If you need to move a project from one organization to another, contact [Pinecone support](https://support.pinecone.io). \n\n\n## Billing settings\n\n\nAll of the projects in an organization share the same billing method and settings. The billing settings for the organization are controlled by the organization owners.\n\n\n## Organization roles\n\n\nThere are two organization roles: organization owner and organization user.\n\n\n### Organization owners\n\n\nOrganization owners manage organization billing, users, and projects. Organization owners are also [project owners](projects#project-roles) for every project belonging to the organization. This means that organization owners have all permissions to manage project members, API keys, and quotas for these projects.\n\n\n### Organization users\n\n\nUnlike organization owners, organization users cannot edit billing settings or invite new users to the organization. Organization users can create new projects, and project owners can add organization members to a project. New users have whatever role the organization owners and project owners grant them. Project owners can add users to a project if those users belong to the same organization as the project.\n\n\n**Table 1: Organization roles and permissions**\n\n\n\n\n| Organization role | Permissions in organization |\n| --- | --- |\n| Organization owner | Project owner for all projects |\n|  | Create projects |\n|  | Manage billing |\n|  | Manags organization members |\n| Organization member | Create projects |\n|  | Join projects when invited |\n|  | Read access to billing |\n\n\n## Organization single sign-on (SSO)\n\n\nSSO allows organizations to manage their teams' access to Pinecone through their identity management solution. Once your integration is configured, you can require that users from your domain sign in through SSO, and you can specify a default role for teammates when they sign up. Only organizations in the enterprise tier can set up SSO. To set up your SSO integration, contact Pinecone support at [[email protected]](/cdn-cgi/l/email-protection#a3d0d6d3d3ccd1d7e3d3cacdc6c0cccdc68dcacc).\n\n\n## Next steps\n\n\n* [Add users to an organization](add-users-to-projects-and-organizations/)\nUpdated 29 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Overview](#overview)\n\t+ [Projects in an organization](#projects-in-an-organization)\n\t+ [Billing settings](#billing-settings)\n\t+ [Organization roles](#organization-roles)\n\t\t- [Organization owners](#organization-owners)\n\t\t- [Organization users](#organization-users)\n\t+ [Organization single sign-on (SSO)](#organization-single-sign-on-sso)\n\t+ [Next steps](#next-steps)\n",
          "# Manage datasets\n\n[Suggest Edits](/edit/datasets)## Overview\n\n\nThis category contains concept topics and guides for tasks related to Pinecone datasets.\n\n\n## Concepts\n\n\n* [Pinecone public datasets](pinecone-public-datasets)\n\n\n## Tasks\n\n\n* [Using public Pinecone datasets](using-public-datasets)\n* [Creating and loading private datasets](creating-datasets)\nUpdated 29 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Overview](#overview)\n\t+ [Concepts](#concepts)\n\t+ [Tasks](#tasks)\n",
          "# Architecture\n\n[Suggest Edits](/edit/architecture)## Overview\n\n\nThis document describes the basic architecture of the Pinecone database.\n\n\n## Basic architecture\n\n\nThe Pinecone vector database is a cloud-based service deployed partly on Kubernetes. Pinecone serves control plane requests from an API gateway and routes these requests to user indexes; clients make data plane requests directly to pods. See Fig. 1 below. \n\n\n**Figure 1: Pinecone architecture diagram**\n\n\n ![Pinecone architecture diagram](https://raw.githubusercontent.com/pinecone-io/img/main/Pinecone%20architecture%20diagram.png)\n\n\n## Components\n\n\n* **Pods**. Each index has one or more replicas, each of which is deployed to a pod with assigned SSD and memory capacity. The CPU assigned to the pod performs computation; the SSD stores metadata.\n* A **stream processor** indexes vectors.\n* The **blob store** contains persistent snapshots of replica data.\n\n\nFor more information about security and encryption, see [Security](security).\n\nUpdated 4 months ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Overview](#overview)\n\t+ [Basic architecture](#basic-architecture)\n\t+ [Components](#components)\n",
          "# Moving to production\n\n[Suggest Edits](/edit/moving-to-production)## Introduction\n\n\nThe goal of this document is to prepare users to begin using their Pinecone indexes in production by anticipating production issues and identifying best practices for production indexes. Because these issues are highly workload-specific, the recommendations here are general.\n\n\n## Overview\n\n\nOnce you have become familiar with Pinecone and experimented with creating indexes and queries that reflect your intended workload, you may be planning to use your indexes to serve production queries. Before you do, there are several steps you can take that can prepare your project for production workloads, anticipate production issues, and enable reliability and growth. \n\n\nConsider the following areas before moving your indexes to production:\n\n\n## Prepare your project structure\n\n\nOne of the first steps towards a production-ready Pinecone index is configuring your project correctly. Consider [creating a separate project](https://www.pinecone.io/docs/manage-projects/#creating-a-new-project) for your development and production indexes, to allow for testing changes to your index before deploying them to production. Ensure that you have properly [configured user access](https://www.pinecone.io/docs/manage-projects/#user-roles) to your production environment so that only those users who need to access the production index can do so. Consider how best to manage the API key associated with your production project.\n\n\n## Test your query results\n\n\nBefore you move your index to production, make sure that your index is returning accurate results in the context of your application. Consider [identifying the appropriate metrics](https://www.pinecone.io/learn/offline-evaluation/) for evaluating your results. \n\n\n## Estimate the appropriate number and size of pods and replicas\n\n\nDepending on your data and the types of workloads you intend to run, your project may require a different [number and size of pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes) and [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas). Factors to consider include the number of vectors, the dimensions per vector, the amount and cardinality of metadata, and the acceptable queries per second (QPS). Use the [index fullness metric](/reference/describe_index_stats_post) to identify how much of your current resources your indexes are using. You can [use collections to create indexes](https://www.pinecone.io/docs/manage-indexes/#create-an-index-from-a-collection) with different pod types and sizes to experiment.\n\n\n## Load test your indexes\n\n\nBefore moving your project to production, consider determining whether your index configuration can serve the load of queries you anticipate from your application. You can write load tests in Python from scratch or using a load testing framework like [Locust](https://locust.io/).\n\n\n## Back up your indexes\n\n\nIn order to enable long-term retention, compliance archiving, and deployment of new indexes, consider backing up your production indexes by [creating collections](https://www.pinecone.io/docs/back-up-indexes/). \n\n\n## Tune for performance\n\n\nBefore serving production workloads, identify ways to [improve latency](https://www.pinecone.io/docs/performance-tuning/) by making changes to your deployment, project configuration, or client. \n\n\n## Configure monitoring\n\n\nPrepare to observe production performance and availability by [configuring monitoring](https://www.pinecone.io/docs/monitoring/) with Prometheus or OpenMetrics on your production indexes.\n\n\n## Plan for scaling\n\n\nBefore going to production, consider planning ahead for how you might scale your indexes when the need arises. Identify metrics that may indicate the need to scale, such as [index fullness](/reference/describe_index_stats_post) and [average request latency](https://www.pinecone.io/docs/monitoring/#average-request-latency). Plan for increasing the number of pods, changing to a more performant [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), [vertically scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1) the [size of your pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), increasing the number of [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas), or increasing storage capacity with a [storage-optimized pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes).\n\n\n## Know how to get support\n\n\nIf you need help, visit [support.pinecone.io](https://support.pinecone.io), or talk to the [Pinecone community](https://www.pinecone.io/community/). Ensure that your [plan tier](https://www.pinecone.io/pricing/) matches the support and availability SLAs you need. This may require you to upgrade to Enterprise.\n\nUpdated 28 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Introduction](#introduction)\n\t+ [Overview](#overview)\n\t+ [Prepare your project structure](#prepare-your-project-structure)\n\t+ [Test your query results](#test-your-query-results)\n\t+ [Estimate the appropriate number and size of pods and replicas](#estimate-the-appropriate-number-and-size-of-pods-and-replicas)\n\t+ [Load test your indexes](#load-test-your-indexes)\n\t+ [Back up your indexes](#back-up-your-indexes)\n\t+ [Tune for performance](#tune-for-performance)\n\t+ [Configure monitoring](#configure-monitoring)\n\t+ [Plan for scaling](#plan-for-scaling)\n\t+ [Know how to get support](#know-how-to-get-support)\n"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "id",
           "type": "string"
          },
          {
           "name": "text",
           "type": "string"
          },
          {
           "name": "source",
           "type": "string"
          },
          {
           "name": "metadata",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728aeea1-1dcf-5d0a-91f2-ecccd4dd4272</td>\n",
       "      <td># Scale indexes\\n\\n[Suggest Edits](/edit/scali...</td>\n",
       "      <td>https://docs.pinecone.io/docs/scaling-indexes</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'scaling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f19f269-171f-5556-93f3-a2d7eabbe50f</td>\n",
       "      <td># Understanding organizations\\n\\n[Suggest Edit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/organizations</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2a71cb3-5148-5090-86d5-7f4156edd7cf</td>\n",
       "      <td># Manage datasets\\n\\n[Suggest Edits](/edit/dat...</td>\n",
       "      <td>https://docs.pinecone.io/docs/datasets</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'datasets'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1dafe68a-2e78-57f7-a97a-93e043462196</td>\n",
       "      <td># Architecture\\n\\n[Suggest Edits](/edit/archit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/architecture</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd</td>\n",
       "      <td># Moving to production\\n\\n[Suggest Edits](/edi...</td>\n",
       "      <td>https://docs.pinecone.io/docs/moving-to-produc...</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'moving-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  ...                                           metadata\n",
       "0  728aeea1-1dcf-5d0a-91f2-ecccd4dd4272  ...  {'created_at': '2023_10_25', 'title': 'scaling...\n",
       "1  2f19f269-171f-5556-93f3-a2d7eabbe50f  ...  {'created_at': '2023_10_25', 'title': 'organiz...\n",
       "2  b2a71cb3-5148-5090-86d5-7f4156edd7cf  ...  {'created_at': '2023_10_25', 'title': 'datasets'}\n",
       "3  1dafe68a-2e78-57f7-a97a-93e043462196  ...  {'created_at': '2023_10_25', 'title': 'archite...\n",
       "4  8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd  ...  {'created_at': '2023_10_25', 'title': 'moving-...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_parquet(\"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9902621e-8561-48b6-8e6d-86a888756c41",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 54,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1722352782799,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "display(Markdown(data.iloc[0]['text']))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Scale indexes\n",
       "\n",
       "[Suggest Edits](/edit/scaling-indexes)In this topic, we explain how you can scale your indexes horizontally and vertically.\n",
       "\n",
       "\n",
       "Projects in the `gcp-starter` environment do not support the features referred to here, including pods, replicas, and collections.\n",
       "\n",
       "\n",
       "## Vertical vs. horizontal scaling\n",
       "\n",
       "\n",
       "If you need to scale your environment to accommodate more vectors, you can modify your existing index to scale it vertically or create a new index and scale horizontally. This article will describe both methods and how to scale your index effectively. \n",
       "\n",
       "\n",
       "## Vertical scaling\n",
       "\n",
       "\n",
       "Scaling vertically is fast and involves no downtime. This is a good choice when you can't pause upserts and must continue serving traffic. It also allows you to double your capacity instantly. However, there are some factors to consider.\n",
       "\n",
       "\n",
       "By [changing the pod size](manage-indexes#changing-pod-sizes), you can scale to x2, x4, and x8 pod sizes, which means you are doubling your capacity at each step. Moving up to a new capacity will effectively double the number of pods used at each step. If you need to scale by smaller increments, then consider horizontal scaling. \n",
       "\n",
       "\n",
       "The number of base pods you specify when you initially create the index is static and cannot be changed. For example, if you start with 10 pods of `p1.x1` and vertically scale to `p1.x2`, this equates to 20 pods worth of usage. Neither can you change pod types with vertical scaling. If you want to change your pod type while scaling, then horizontal scaling is the better option. \n",
       "\n",
       "\n",
       "You can only scale index sizes up and cannot scale them back down.\n",
       "\n",
       "\n",
       "See our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\n",
       "\n",
       "\n",
       "## Horizontal scaling\n",
       "\n",
       "\n",
       "There are two approaches to horizontal scaling in Pinecone: adding pods and adding replicas. Adding pods increases all resources but requires a pause in upserts; adding replicas only increases throughput and requires no pause in upserts.\n",
       "\n",
       "\n",
       "### Adding pods\n",
       "\n",
       "\n",
       "Adding pods to an index increases all resources, including available capacity. Adding pods to an existing index is possible using our <collections> feature. A collection is an immutable snapshot of your index in time: a collection stores the data but not the original index definition.\n",
       "\n",
       "\n",
       "When you [create an index from a collection](manage-indexes#create-an-index-from-a-collection), you define the new index configuration. This allows you to scale the base pod count horizontally without scaling vertically. The main advantage of this approach is that you can scale incrementally instead of doubling capacity as with vertical scaling. Also, you can redefine pod types if you are experimenting or if you need to use a different pod type, such asperformance-optimized pods or storage-optimized pods. Another advantage of this method is that you can change your [metadata configuration](manage-indexes#selective-metadata-indexing) to redefine metadata fields as indexed or stored-only. This is important when [tuning your index](performance-tuning) for the best throughput. \n",
       "\n",
       "\n",
       "Here are the general steps to make a copy of your index and create a new index while changing the pod type, pod count, metadata configuration, replicas, and all typical parameters when creating a new collection: \n",
       "\n",
       "\n",
       "1. Pause upserts.\n",
       "2. Create a collection from the current index.\n",
       "3. Create an index from the collection with new parameters.\n",
       "4. Continue upserts to the newly created index. Note: the URL has likely changed.\n",
       "5. Delete the old index if desired.\n",
       "\n",
       "\n",
       "### Adding replicas\n",
       "\n",
       "\n",
       "Each replica duplicates the resources and data in an index. This means that adding additional replicas increases the throughput of the index but not its capacity. However, adding replicas does not require downtime.\n",
       "\n",
       "\n",
       "Throughput in terms of queries per second (QPS) scales linearly with the number of replicas per index.\n",
       "\n",
       "\n",
       "To add replicas, use the `configure_index` operation to [increase the number of replicas for your index](manage-indexes#replicas).\n",
       "\n",
       "\n",
       "## Next steps\n",
       "\n",
       "\n",
       "* See our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\n",
       "* Learn more about <collections>.\n",
       "Updated 29 days ago \n",
       "\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "* [Table of Contents](#)\n",
       "* + [Vertical vs. horizontal scaling](#vertical-vs-horizontal-scaling)\n",
       "\t+ [Vertical scaling](#vertical-scaling)\n",
       "\t+ [Horizontal scaling](#horizontal-scaling)\n",
       "\t\t- [Adding pods](#adding-pods)\n",
       "\t\t- [Adding replicas](#adding-replicas)\n",
       "\t+ [Next steps](#next-steps)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(data.iloc[0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1044ae4-768c-404f-a943-48f950d4702b",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0949610-9b4f-4066-8d46-05624908d899",
   "metadata": {},
   "source": [
    "### Markdown Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e688ef3-72d2-4362-bfa1-9c65e19fcb31",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 52,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1722353029798,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from typing import List, Tuple\nimport marko\nfrom marko.md_renderer import MarkdownRenderer\n\n# Custom renderer class to add a newline after each paragraph\nclass CustomMarkdownRenderer(MarkdownRenderer):\n    def render_paragraph(self, element):\n        return super().render_paragraph(element) + \"\\n\"\n\n# Function to create chunks from markdown text\ndef markdown_chunker(markdown_text: str, max_chunk_size: int = 1000) -> List[str]:\n    # Parse the markdown text\n    parsed = marko.parse(markdown_text)\n    # Initialize the custom markdown renderer\n    md_renderer = CustomMarkdownRenderer()\n\n    # Function to process each element in the parsed markdown\n    def process_element(element) -> List[Tuple[str, bool]]:\n        if isinstance(element, str):\n            return [(element, False)]\n        \n        # Render the element to markdown text\n        rendered_text = md_renderer.render(element) + \"\\n\"\n        # Check if the element is a heading\n        is_heading = isinstance(element, marko.block.Heading)\n        \n        result = [(rendered_text, is_heading)]\n        \n        # Recursively process child elements if they exist\n        if hasattr(element, 'children'):\n            for child in element.children:\n                result.extend(process_element(child))\n        \n        return result\n\n    # Function to combine processed elements into chunks\n    def combine_chunks(elements: List[Tuple[str, bool]]) -> List[str]:\n        chunks = []\n        current_chunk = \"\"\n        \n        for text, is_heading in elements:\n            # If the current element is a heading and there's content in the current chunk, finalize the current chunk\n            if is_heading and current_chunk:\n                chunks.append(current_chunk.strip())\n                current_chunk = \"\"\n            \n            # If adding the current text exceeds the max chunk size, finalize the current chunk\n            if len(current_chunk) + len(text) > max_chunk_size and current_chunk:\n                chunks.append(current_chunk.strip())\n                current_chunk = \"\"\n            \n            # Add the current text to the current chunk\n            current_chunk += text\n\n        # Add any remaining content as the last chunk\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        # Remove duplicate chunks\n        unique_chunks = []\n        seen_chunks = set()\n        for chunk in chunks:\n            if chunk not in seen_chunks:\n                unique_chunks.append(chunk)\n                seen_chunks.add(chunk)\n        \n        return unique_chunks\n\n    # Process the parsed markdown elements\n    processed_elements = process_element(parsed)\n    # Combine the processed elements into chunks\n    return combine_chunks(processed_elements[1:])"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import marko\n",
    "from marko.md_renderer import MarkdownRenderer\n",
    "\n",
    "# Custom renderer class to add a newline after each paragraph\n",
    "class CustomMarkdownRenderer(MarkdownRenderer):\n",
    "    def render_paragraph(self, element):\n",
    "        return super().render_paragraph(element) + \"\\n\"\n",
    "\n",
    "# Function to create chunks from markdown text\n",
    "def markdown_chunker(markdown_text: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "    # Parse the markdown text\n",
    "    parsed = marko.parse(markdown_text)\n",
    "    # Initialize the custom markdown renderer\n",
    "    md_renderer = CustomMarkdownRenderer()\n",
    "\n",
    "    # Function to process each element in the parsed markdown\n",
    "    def process_element(element) -> List[Tuple[str, bool]]:\n",
    "        if isinstance(element, str):\n",
    "            return [(element, False)]\n",
    "        \n",
    "        # Render the element to markdown text\n",
    "        rendered_text = md_renderer.render(element) + \"\\n\"\n",
    "        # Check if the element is a heading\n",
    "        is_heading = isinstance(element, marko.block.Heading)\n",
    "        \n",
    "        result = [(rendered_text, is_heading)]\n",
    "        \n",
    "        # Recursively process child elements if they exist\n",
    "        if hasattr(element, 'children'):\n",
    "            for child in element.children:\n",
    "                result.extend(process_element(child))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Function to combine processed elements into chunks\n",
    "    def combine_chunks(elements: List[Tuple[str, bool]]) -> List[str]:\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for text, is_heading in elements:\n",
    "            # If the current element is a heading and there's content in the current chunk, finalize the current chunk\n",
    "            if is_heading and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            \n",
    "            # If adding the current text exceeds the max chunk size, finalize the current chunk\n",
    "            if len(current_chunk) + len(text) > max_chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            \n",
    "            # Add the current text to the current chunk\n",
    "            current_chunk += text\n",
    "\n",
    "        # Add any remaining content as the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        # Remove duplicate chunks\n",
    "        unique_chunks = []\n",
    "        seen_chunks = set()\n",
    "        for chunk in chunks:\n",
    "            if chunk not in seen_chunks:\n",
    "                unique_chunks.append(chunk)\n",
    "                seen_chunks.add(chunk)\n",
    "        \n",
    "        return unique_chunks\n",
    "\n",
    "    # Process the parsed markdown elements\n",
    "    processed_elements = process_element(parsed)\n",
    "    # Combine the processed elements into chunks\n",
    "    return combine_chunks(processed_elements[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ec6fe-e253-4009-a77e-7b375c78f052",
   "metadata": {},
   "source": [
    "### Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8891fa0-7bcc-4b6c-b5dd-811880519c09",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 1658,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1722307535067,
    "lastExecutedByKernel": "f022e413-c9f3-443a-a31f-97387011ca11",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from typing import List\nimport re\n\ndef recursive_chunker(text: str, min_chunk_size: int = 100, max_chunk_size: int = 500) -> List[str]:\n    # Function to split text into sentences using regular expressions\n    def split_into_sentences(text: str) -> List[str]:\n        sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n        return sentence_endings.split(text)\n\n    # Function to split text into paragraphs based on double newlines\n    def split_into_paragraphs(text: str) -> List[str]:\n        return text.split('\\n\\n')\n\n    # Function to chunk text into smaller pieces based on min and max sizes\n    def chunk_text(text: str, min_size: int, max_size: int) -> List[str]:\n        # Split the text into paragraphs\n        paragraphs = split_into_paragraphs(text)\n        chunks = []\n        current_chunk = \"\"\n\n        for paragraph in paragraphs:\n            # If adding the paragraph to the current chunk does not exceed max_size, add it\n            if len(current_chunk) + len(paragraph) <= max_size:\n                current_chunk += paragraph + \"\\n\\n\"\n            else:\n                # If the current chunk is not empty, add it to the chunks list\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                \n                # If the paragraph itself is larger than max_size, split it into sentences\n                if len(paragraph) > max_size:\n                    sentences = split_into_sentences(paragraph)\n                    for sentence in sentences:\n                        # If the sentence fits within max_size, add it to the current chunk\n                        if len(sentence) <= max_size:\n                            if len(current_chunk) + len(sentence) > max_size:\n                                # If adding the sentence exceeds max_size, finalize the current chunk and start a new one\n                                if current_chunk:\n                                    chunks.append(current_chunk.strip())\n                                current_chunk = sentence + \" \"\n                            else:\n                                current_chunk += sentence + \" \"\n                        else:\n                            # If the sentence itself is larger than max_size, add it as a separate chunk\n                            if current_chunk:\n                                chunks.append(current_chunk.strip())\n                            chunks.append(sentence.strip())\n                            current_chunk = \"\"\n                else:\n                    # If the paragraph fits within max_size, start a new chunk with this paragraph\n                    current_chunk = paragraph + \"\\n\\n\"\n\n        # Add any remaining text in the current chunk to the chunks list\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n\n        # Return only the chunks that meet the minimum size requirement\n        return [chunk for chunk in chunks if len(chunk) >= min_size]\n\n    final_chunks = []\n    to_process = [text]\n\n    # Process the text recursively to create chunks\n    while to_process:\n        current_text = to_process.pop(0)\n        if len(current_text) <= max_chunk_size:\n            if len(current_text) >= min_chunk_size:\n                final_chunks.append(current_text)\n        else:\n            new_chunks = chunk_text(current_text, min_chunk_size, max_chunk_size)\n            to_process.extend(new_chunks)\n\n    return final_chunks"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def recursive_chunker(text: str, min_chunk_size: int = 100, max_chunk_size: int = 500) -> List[str]:\n",
    "    # Function to split text into sentences using regular expressions\n",
    "    def split_into_sentences(text: str) -> List[str]:\n",
    "        # Regular expression to identify sentence endings\n",
    "        sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "        return sentence_endings.split(text)\n",
    "\n",
    "    # Function to split text into paragraphs based on double newline characters\n",
    "    def split_into_paragraphs(text: str) -> List[str]:\n",
    "        return text.split('\\n\\n')\n",
    "\n",
    "    # Function to chunk text into smaller pieces based on min and max size constraints\n",
    "    def chunk_text(text: str, min_size: int, max_size: int) -> List[str]:\n",
    "        paragraphs = split_into_paragraphs(text)  # Split text into paragraphs\n",
    "        chunks = []  # List to store the resulting chunks\n",
    "        current_chunk = \"\"  # Variable to accumulate the current chunk\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # If adding the paragraph to the current chunk doesn't exceed max size, add it\n",
    "            if len(current_chunk) + len(paragraph) <= max_size:\n",
    "                current_chunk += paragraph + \"\\n\\n\"\n",
    "            else:\n",
    "                # If the current chunk is not empty, add it to the chunks list\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                \n",
    "                # If the paragraph itself is larger than max size, split it into sentences\n",
    "                if len(paragraph) > max_size:\n",
    "                    sentences = split_into_sentences(paragraph)\n",
    "                    for sentence in sentences:\n",
    "                        # If the sentence fits within max size, add it to the current chunk\n",
    "                        if len(sentence) <= max_size:\n",
    "                            if len(current_chunk) + len(sentence) > max_size:\n",
    "                                if current_chunk:\n",
    "                                    chunks.append(current_chunk.strip())\n",
    "                                current_chunk = sentence + \" \"\n",
    "                            else:\n",
    "                                current_chunk += sentence + \" \"\n",
    "                        else:\n",
    "                            # If the sentence is too large, split it into smaller pieces\n",
    "                            if current_chunk:\n",
    "                                chunks.append(current_chunk.strip())\n",
    "                            chunks.extend([sentence[i:i+max_size] for i in range(0, len(sentence), max_size)])\n",
    "                            current_chunk = \"\"\n",
    "                else:\n",
    "                    current_chunk = paragraph + \"\\n\\n\"\n",
    "\n",
    "        # Add any remaining text in the current chunk to the chunks list\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        # Filter out chunks that are smaller than the minimum size\n",
    "        return [chunk for chunk in chunks if len(chunk) >= min_size]\n",
    "\n",
    "    # Initial chunking of the text\n",
    "    chunks = chunk_text(text, min_chunk_size, max_chunk_size)\n",
    "    \n",
    "    # Additional pass to ensure all chunks are within size limits\n",
    "    final_chunks = []\n",
    "    seen = set()  # Set to keep track of seen chunks to avoid duplicates\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) <= max_chunk_size:\n",
    "            if chunk not in seen:\n",
    "                final_chunks.append(chunk)\n",
    "                seen.add(chunk)\n",
    "        else:\n",
    "            # If a chunk is too large, re-chunk it\n",
    "            for sub_chunk in chunk_text(chunk, min_chunk_size, max_chunk_size):\n",
    "                if sub_chunk not in seen:\n",
    "                    final_chunks.append(sub_chunk)\n",
    "                    seen.add(sub_chunk)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1454ee9-f3e6-4c54-a72d-61a236f036c5",
   "metadata": {},
   "source": [
    "## Embedding and upserting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee4b93-3092-44ae-a77c-eebedb0fcdc3",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f253514-e275-4341-85f9-9c46d5987e83",
   "metadata": {},
   "source": [
    "Initalize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d9fd4c-5fe7-4fb6-9571-bf0a65b97572",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1722353083657,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from openai import OpenAI\nfrom pinecone import Pinecone, ServerlessSpec\n\n# Initialize OpenAI client\nopenai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n# Initialize Pinecone client\npinecone_client = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))",
    "outputsMetadata": {
     "0": {
      "height": 164,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone_client = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742387d-2d28-48e0-bed3-39c3d90d905c",
   "metadata": {},
   "source": [
    "Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5a5c31-e00a-45fb-9fdf-0c28c303ee53",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1722353180254,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "index_name = \"markdown-chunks\""
   },
   "outputs": [],
   "source": [
    "index_name = \"markdown-chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b71b13d-8f7d-435e-bc44-23878dffa262",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 239,
    "lastExecutedAt": 1722353182118,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create or get existing index\nif index_name not in [index['name'] for index in pinecone_client.list_indexes()]:\n    pinecone_client.create_index(\n        name=index_name,\n        dimension=1536,  # OpenAI embeddings are 1536 dimensions\n        metric='cosine',\n        spec=ServerlessSpec(cloud='aws', region='us-west-2')\n    )\n\nindex = pinecone_client.Index(index_name)"
   },
   "outputs": [],
   "source": [
    "# Create or get existing index\n",
    "if index_name not in [index['name'] for index in pinecone_client.list_indexes()]:\n",
    "    pinecone_client.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI embeddings are 1536 dimensions\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "    )\n",
    "\n",
    "index = pinecone_client.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8ae31-95ff-4464-a619-7983cc546c68",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc334b1-8ca0-4292-b398-9d3219444933",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 175,
    "lastExecutedAt": 1725567736473,
    "lastExecutedByKernel": "531daebc-691b-44ef-a578-86afed339eb1",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def embed_and_upsert_chunk_with_metadata(chunk, metadata):\n    # Generate embedding\n    response = openai_client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=chunk,\n    )\n    embedding = response.data[0].embedding\n\n    # Upsert to Pinecone\n    index.upsert(\n        vectors=[(f\"chunk_{metadata['id']}\", embedding, metadata)],\n    )"
   },
   "outputs": [],
   "source": [
    "def embed_and_upsert_chunk_with_metadata(chunk, metadata):\n",
    "    # Generate embedding\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=chunk,\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    # Upsert to Pinecone\n",
    "    index.upsert(\n",
    "        vectors=[(f\"chunk_{metadata['id']}\", embedding, metadata)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c5209-a254-4b79-af02-84a6cb017a4b",
   "metadata": {},
   "source": [
    "### Upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b40dd11e-f631-444a-aaaf-f94f26feb8ef",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 434022,
    "lastExecutedAt": 1722312131611,
    "lastExecutedByKernel": "62963ff9-85fd-4875-83b9-0036869b36c7",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "chunks_with_metadata = []\n\nfor idx, row in data.iterrows():\n    text_chunks = markdown_chunker(row['text'])\n    for cidx, chunk in enumerate(text_chunks):\n        metadata = row['metadata'].copy()\n        metadata['id'] = f\"{row['id']}_{cidx}\"\n        metadata['chunk_text'] = chunk\n        chunks_with_metadata.append((chunk, metadata))\n        \n\nfor chunk, metadata in chunks_with_metadata:\n    embed_and_upsert_chunk_with_metadata(chunk, metadata)"
   },
   "outputs": [],
   "source": [
    "chunks_with_metadata = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    text_chunks = markdown_chunker(row['text'])\n",
    "    for cidx, chunk in enumerate(text_chunks):\n",
    "        metadata = row['metadata'].copy()\n",
    "        metadata['id'] = f\"{row['id']}_{cidx}\"\n",
    "        metadata['chunk_text'] = chunk\n",
    "        chunks_with_metadata.append((chunk, metadata))\n",
    "        \n",
    "\n",
    "for chunk, metadata in chunks_with_metadata:\n",
    "    embed_and_upsert_chunk_with_metadata(chunk, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff59f3-5dc6-4d81-9448-d8c37b083e0f",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1d3ef-9258-4cf6-8a12-5c1ae33763d0",
   "metadata": {},
   "source": [
    "### Naive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a65716-4adb-4ffe-ada6-0a10885d9ad2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1722353512438,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def retrieve_chunks(query_text, num_chunks=3):\n    query_embedding = openai_client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=query_text\n    ).data[0].embedding\n    \n    results = index.query(vector=query_embedding, top_k=num_chunks, include_metadata=True)\n\n    return [chunk['metadata']['chunk_text'] for chunk in results.matches]\n    "
   },
   "outputs": [],
   "source": [
    "def retrieve_chunks(query_text, num_chunks=3):\n",
    "    query_embedding = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=query_text\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    results = index.query(vector=query_embedding, top_k=num_chunks, include_metadata=True)\n",
    "\n",
    "    return [chunk['metadata']['chunk_text'] for chunk in results.matches]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74229a2d-df59-4fe1-b676-8caa4063e139",
   "metadata": {},
   "source": [
    "### Retrieval with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "136517a2-e773-445e-bcad-620854ba061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerankers import Reranker, Document\n",
    "\n",
    "def retrieve_chunks(query_text, num_chunks=3):\n",
    "    query_embedding = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=query_text\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    # Increase the number of chunks to 2 times the number of chunks requested\n",
    "    results = index.query(vector=query_embedding, top_k=num_chunks * 2, include_metadata=True)\n",
    "    \n",
    "    try:\n",
    "        # Initialize reranker with a specific model\n",
    "        ranker = Reranker('flashrank')\n",
    "        # Prepare documents for reranking\n",
    "        docs = [Document(text=chunk['metadata']['chunk_text'], doc_id=i) for i, chunk in enumerate(results.matches)]\n",
    "        # Rerank the documents\n",
    "        reranked_results = ranker.rank(query=query_text, docs=docs)\n",
    "        # Return the top num_chunks reranked results\n",
    "        return [result.document.text for result in reranked_results.top_k(num_chunks)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieve_chunks: {e}\")\n",
    "        # Fallback to original results if reranking fails\n",
    "        return [chunk['metadata']['chunk_text'] for chunk in results.matches[:num_chunks]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1344cf-3f4a-4003-ae92-7ecee8618841",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8c9d1b0-2bd0-42ac-b060-7237d720c69e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1722354361690,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def augmented_query(query_text, context, num_chunks=5):\n  prompt = f\"\"\"\n  Given the following context, answer the following question.\n  Context: {context}\n  Query: {query_text}\n  \"\"\"\n  response = openai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n      {\"role\": \"system\", \"content\": prompt}\n    ]\n  )\n\n  return response.choices[0].message.content"
   },
   "outputs": [],
   "source": [
    "def augmented_query(query_text, context, num_chunks=5):\n",
    "  prompt = f\"\"\"\n",
    "  Given the following context, answer the following question.\n",
    "  Context: {context}\n",
    "  Query: {query_text}\n",
    "  \"\"\"\n",
    "  response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc9f5d-cdd8-4d20-a50c-4d04c449340b",
   "metadata": {},
   "source": [
    "### E2E RAG Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2df29f-6fa3-41f7-8fc9-b2e262ca6bca",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1722353724429,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def rag_flow():\n    user_input = input(\"Please enter your query: \")\n    context = retrieve_chunks(user_input)    \n    augmented_query_result = augmented_query(user_input, \"\\n\".join(context))\n    display(Markdown(f\"**Answer:** {augmented_query_result}\"))\n  "
   },
   "outputs": [],
   "source": [
    "def rag_flow():\n",
    "    user_input = input(\"Please enter your query: \")\n",
    "    context = retrieve_chunks(user_input)    \n",
    "    augmented_query_result = augmented_query(user_input, \"\\n\".join(context))\n",
    "    display(Markdown(f\"**Answer:** {augmented_query_result}\"))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5c343c-89c4-47c6-b3c6-cd040e7115d2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4791,
    "lastExecutedAt": 1722353733408,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "rag_flow()",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 164,
      "type": "stream"
     },
     "2": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query: How do you create an index in Pinecone using Python?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** To create an index in Pinecone using Python, you can use the `pinecone.create_index()` method. In the provided context, the index is created with the name `langchain-retrieval-augmentation`, a cosine similarity metric, and a dimension of 1536 (taken from the length of `res[0]`).\r\r\n",
       "\r\r\n",
       "The specific code snippet for creating the index would look like this:\n",
       "\n",
       "```python\n",
       "index_name = 'langchain-retrieval-augmentation'\n",
       "pinecone.create_index(\n",
       "    name=index_name,\n",
       "    metric='cosine',\n",
       "    dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
       ")\n",
       "```\n",
       "\n",
       "After creating the index, you can access it using `index = pinecone.Index(index_name)`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b4c41-be42-4a1d-a2af-3376a98fdb2e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce366664-0ebf-4f21-b913-01673a677e8f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 187,
    "lastExecutedAt": 1722353926242,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.docstore.document import Document\n"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910d387-b4b6-49a1-b1b0-82025cd4b418",
   "metadata": {},
   "source": [
    "### Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e66a789a-8bd5-4235-93bb-2c97ddd8e9c0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1722353974850,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "documents = []\nfor _, row in data.iterrows():\n    text = row['text']\n    metadata = {\n        'id': row['id'],\n        'metadata': row['metadata'],\n        'filename': row['source']\n    }\n    doc = Document(page_content=text, metadata=metadata)\n    documents.append(doc)"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for _, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    metadata = {\n",
    "        'id': row['id'],\n",
    "        'metadata': row['metadata'],\n",
    "        'filename': row['source']\n",
    "    }\n",
    "    doc = Document(page_content=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69537494-27e6-4ad4-9358-6457c9edd1e6",
   "metadata": {},
   "source": [
    "### Test set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ad087a7-3ceb-45f2-90f1-ea601da5199b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 69570,
    "lastExecutedAt": 1722354099040,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\n# generate testset\ntestset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45f9ff159cc4a0cacc4dcad487f2b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae224d6913c48e0824431a3bd6fdaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52ac3b3f-05c5-4ed5-af97-1b1cce420a07",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 63,
    "lastExecutedAt": 1722354149830,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "testset.to_pandas().head()\n",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "contexts": [
          [
           "s\": batch}\n    )\n    emb = res.json()['embeddings']\n    # get metadata (just the original text)\n    meta = [{'text': text} for text in batch]\n    # create IDs\n    ids = [str(x) for x in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n\n```\n\n\n```python\n100%|██████████| 782/782 [11:02<00:00,  1.18it/s]\n\n\n{'dimension': 768,\n 'index_fullness': 0.1,\n 'namespaces': {'': {'vector_count': 50000}},\n 'total_vector_count': 50000}\n\n```\n\nWith everything indexed we can begin querying. We will take a few examples from the *premise* column of the dataset.\n\n\nPython\n```python\nquery = snli['premise'][0]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n\n```\n\n\n```python\nQuery: A person on a horse jumps over a broken down airplane.\nAnswers:\nThe horse jumps over a toy airplane.\na lady rides a horse over a plane shaped obstacle\nA person getting onto a horse.\nperson rides horse\nA woman riding a horse jumps over a bar.\n\n```\n\nThese look good, let's try a couple more examples.\n\n\nPython\n```python\nquery = snli['premise'][100]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n\n```\n\n\n```python\nQuery: A woman is walking across the street eating a banana, while a man is following with his briefcase.\nAnswers:\nA woman eats a banana and walks across a street, and there is a man trailing behind her.\nA woman eats a banana split.\nA woman is carrying two small watermelons and a purse while walking down the street.\nThe woman walked across the street.\nA woman walking on the street with a monkey on her back.\n\n```\n\nAnd one more...\n\n\nPython\n```python\nquery = snli['premise'][200]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n\n```\n\n\n```python\nQuery: People on bicycles waiting at an intersection.\nAnswers:\nA pair of people on bikes are waiting at a stoplight.\nBike riders wait to cross the street.\npeople on bicycles\nGroup of bike riders stopped in the street.\nThere are bicycles outside.\n\n```\n\nAll of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.",
           "\n\n\n**Once the index is deleted, you cannot use it again.**\n\n\nShut down the endpoint by navigating to the Inference Endpoints **Overview** page and selecting **Delete endpoint**. Delete the Pinecone index with:\n\n\nPython\n```python\npinecone.delete_index(index_name)\n\n```\n\n\n\n---\n\nUpdated 28 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Endpoints](#endpoints)\n\t+ [Creating Embeddings](#creating-embeddings)\n\t+ [Vector DB](#vector-db)\n\t+ [Create and Index Embeddings](#create-and-index-embeddings)\n"
          ],
          [
           "# Insert data\n\n[Suggest Edits](/edit/insert-data)After creating a Pinecone index, you can start inserting vector embeddings and metadata into the index.\n\n\n\n## Inserting records\n\n\n1. Create a client instance and target an index:\n\n\nPythonJavaScriptcurl\n```python\nimport pinecone\n\npinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\")\nindex = pinecone.Index(\"pinecone-index\")\n\n```\n\n```python\nimport { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: \"YOUR_API_KEY\",\n  environment: \"YOUR_ENVIRONMENT\"\n})\nconst index = pinecone.index(\"pinecone-index\")\n\n```\n\n```python\n# Not applicable\n\n```\n\n2. Use the upsert operation to write records into the index:\n\n\nPythonJavaScriptcurl\n```python\n# Insert sample data (5 8-dimensional vectors)\nindex.upsert([\n    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),\n    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]),\n    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]),\n    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]),\n    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n])\n\n```\n\n```python\n const records = [\n    {\n      id: 'A',\n      values: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    },\n    {\n      id: 'B',\n      values: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n    },\n    {\n      id: 'C',\n      values: [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n    },\n    {\n      id: 'D',\n      values: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n    },\n    {\n      id: 'E',\n      values: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n    }\n]\n\nawait index.upsert(records);\n\n```\n\n```python\nPINECONE_ENVIRONMENT='your-environment'\nPINECONE_API_KEY='your-api-key'\nINDEX_NAME='index-name'\n\nPINECONE_PROJECT_ID=$(\n  curl \"https://controller.$PINECONE_ENVIRONMENT.pinecone.io/actions/whoami\" \\\n    -H \"Api-Key: $PINECONE_API_KEY\" | jq -r '.project_name'\n)\n\ncurl -X POST \"https://$INDEX_NAME-$PINECONE_PROJECT_ID.svc.$PINECONE_ENVIRONMENT.pinecone.io/vectors/upsert\" \\\n  -H \"Api-Key: $PINECONE_API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n     "
          ],
          [
           " 'namespace': ''}\n\n```\n\ncurl\n```python\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}},\n    \"topK\": 1,\n    \"includeMetadata\": true\n  }'\n\n# Output:\n# {\n#       \"matches\": [\n#         {\n#           \"id\": \"B\",\n#           \"score\": 0.0800000429,\n#           \"values\": [],\n#           \"metadata\": {\n#             \"genre\": \"documentary\",\n#             \"year\": 2019\n#           }\n#         }\n#       ],\n#       \"namespace\": \"\"\n#     }\n\n```\n\n### More example filter expressions\n\n\nA comedy, documentary, or drama:\n\n\nJSON\n```python\n{\n  \"genre\": { \"$in\": [\"comedy\", \"documentary\", \"drama\"] }\n}\n\n```\n\nA drama from 2020:\n\n\nJSON\n```python\n{\n  \"genre\": { \"$eq\": \"drama\" },\n  \"year\": { \"$gte\": 2020 }\n}\n\n```\n\nA drama from 2020 (equivalent to the previous example):\n\n\nJSON\n```python\n{\n  \"$and\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n```\n\nA drama or a movie from 2020:\n\n\nJSON\n```python\n{\n  \"$or\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n```\n\n## Deleting vectors by metadata filter\n\n\nTo specify vectors to be deleted by metadata values, pass a metadata filter expression to the delete operation. This deletes all vectors matching the metadata filter expression.\n\n\nProjects in the `gcp-starter` region do not support deleting by metadata.\n\n\n**Example**\n\n\nThis example deletes all vectors with genre \"documentary\" and year 2019 from an index.\n\n\nPythonJavaScriptcurl\n```python\nindex.delete(\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    }\n)\n\n```\n\n```python\nawait index._delete({\n  deleteRequest: {\n    filter: {\n    genre: { $eq: \"documentary\" },\n    year: 2019,\n  }\n});\n\n```\n\n```python\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/delete \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}}\n  }'\n\n```\nUpdated 29 days ago \n\n\n\n---\n\n* [Table of Contents](#)\n* + [Supported metadata types](#supported-metadata-types)\n\t+ [Supported metadata size](#supported-metadata-size)\n\t+ [Metadata query language](#metadata-query-language)\n\t\t- [Using arrays of strings as metadata values or as metadata filters](#"
          ],
          [
           "4]\n  },\n  {\n    \"id\": \"E\",\n    \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n  }\n]);\n\n```\n\n```python\nPINECONE_ENVIRONMENT='your-environment'\nPINECONE_API_KEY='your-api-key'\n\nPINECONE_PROJECT_ID=$(\n  curl \"https://controller.$PINECONE_ENVIRONMENT.pinecone.io/actions/whoami\" \\\n    -H \"Api-Key: $PINECONE_API_KEY\" | jq -r '.project_name'\n)\n\ncurl -X POST \"https://quickstart-$PINECONE_PROJECT_ID.svc.$PINECONE_ENVIRONMENT.pinecone.io/vectors/upsert\" \\\n  -H \"Api-Key: $PINECONE_API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"A\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n      },\n      {\n        \"id\": \"B\",\n        \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n      },\n      {\n        \"id\": \"C\",\n        \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n      },\n      {\n        \"id\": \"D\",\n        \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n      },\n      {\n        \"id\": \"E\",\n        \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n      }\n    ]\n  }'\n\n```\n\nThe cURL command above uses the [endpoint](manage-data/#specify-an-index-endpoint) for your Pinecone index. \n\n\n\n> ℹ️Note\n> \n> \n> \n> When upserting larger amounts of data, [upsert data in batches](insert-data/#batching-upserts) of 100 vectors or fewer over multiple upsert requests.\n> \n> \n\n\n6. Get statistics about your index.\n\n\nThe following commands return statistics about the contents of your index.\n\n\nPythonJavaScriptcurl\n```python\nindex.describe_index_stats()\n# Returns:\n# {'dimension': 8, 'index_fullness': 0.0, 'namespaces': {'': {'vector_count': 5}}}\n\n```\n\n```python\nconst indexStats = await index.describeIndexStats();\n// Returns:\n// {\n//   namespaces: { '': { recordCount: 5 } },\n//   dimension: 8,\n//   indexFullness: 0,\n//   totalRecordCount: 5\n// }\n\n```\n\n```python\nPINECONE_ENVIRONMENT='your-environment'\nPINECONE_API_KEY='your-api-key'\n\nPINECONE_PROJECT_ID=$(\n  curl \"https://controller.$PINECONE_ENVIRONMENT.pinecone.io/actions/whoami\" \\\n    -H \"Api-Key: $PINECONE_API_KEY\" | jq -r '.project_name'\n)\n\ncurl -X POST \"https"
          ],
          [
           "#              'id-2': {'id': 'id-2',\n#                       'values': [0.00891787093, 0.581895, 0.315718859, ...]}}}\n\n```\n\n```python\nconst fetchResult = await index.fetch(['id-1', 'id-2']);\n// Returns:\n// {'namespace': '',\n//  'records': {'id-1': {'id': 'id-1',\n//                       'values': [0.568879, 0.632687092, 0.856837332, ...]},\n//              'id-2': {'id': 'id-2',\n//                       'values': [0.00891787093, 0.581895, 0.315718859, ...]}}}\n\n```\n\n```python\nPINECONE_ENVIRONMENT='your-environment'\nPINECONE_API_KEY='your-api-key'\nINDEX_NAME='index-name'\n\nPINECONE_PROJECT_ID=$(\n  curl \"https://controller.$PINECONE_ENVIRONMENT.pinecone.io/actions/whoami\" \\\n    -H \"Api-Key: $PINECONE_API_KEY\" | jq -r '.project_name'\n)\n\ncurl \"https://$INDEX_NAME-$PINECONE_PROJECT_ID.svc.$PINECONE_ENVIRONMENT.pinecone.io/vectors/fetch?ids=id-1&ids=id-2\" \\\n  -H \"Api-Key: $PINECONE_API_KEY\"\n# Output:\n# {\n#   \"vectors\": {\n#     \"id-1\": {\n#       \"id\": \"id-1\",\n#       \"values\": [0.568879, 0.632687092, 0.856837332, ...]\n#     },\n#     \"id-2\": {\n#       \"id\": \"id-2\",\n#       \"values\": [0.00891787093, 0.581895, 0.315718859, ...]\n#     }\n#   },\n#   \"namespace\": \"\"\n# }\n\n```\n\n## Updating records\n\n\nThere are two methods for updating records and metadata, using *full* or *partial* updates.\n\n\n### Full update\n\n\nFull updates modify the entire record, including vector values and metadata. Updating a record by id is done the same way as [inserting records](insert-data). (Write operations in Pinecone are [idempotent](https://en.wikipedia.org/wiki/Idempotence).)\n\n\nThe `Upsert` operation writes records into an index.\n\n\n\n> ℹ️Note\n> \n> \n> \n> If a new value is upserted for an existing vector id, it will overwrite the  \n> \n> previous value.\n> \n> \n\n\n1. Update the value of the record `(\"id-3\", [3.3, 3.3])`:\n\n\nPythonJavaScriptcurl\n```python\nindex.upsert([(\"id-3\", [3.3, 3.3])])\n\n```\n\n```python\nawait index.upsert([\n  { id: '3', values: [3.3, 3.3] }\n]);\n\n```\n\n```python\nINDEX_NAME='index-name'\nPINECONE_ENVIRONMENT='your-environment'\nPINECONE_API_KEY='your-api-key'\n\nPINECONE_PROJECT_ID=$(\n  curl \"https://controller.$PINECONE_ENVIRONMENT.pinecone.io/actions/whoami\" \\\n    -H \"Api-Key: $PINEC"
          ]
         ],
         "episode_done": [
          true,
          true,
          true,
          true,
          true
         ],
         "evolution_type": [
          "simple",
          "simple",
          "simple",
          "simple",
          "simple"
         ],
         "ground_truth": [
          "To shut down the inference endpoint, navigate to the Inference Endpoints Overview page and select Delete endpoint. Additionally, delete the Pinecone index using the command: pinecone.delete_index(index_name).",
          "The purpose of the upsert operation in the context of inserting records into a Pinecone index is to write records into the index, allowing for the insertion of vector embeddings and metadata.",
          "The purpose of genre filtering in the context of querying data is to specify which genres of data (such as comedy, documentary, or drama) should be included in the query results. This allows users to narrow down their search to specific types of content based on their preferences.",
          "The recommended method for upserting larger amounts of data in Pinecone is to upsert data in batches of 100 vectors or fewer over multiple upsert requests.",
          "The two methods for updating records in Pinecone are full updates and partial updates."
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ],
         "metadata": [
          [
           {
            "filename": "https://docs.pinecone.io/docs/hugging-face-endpoints",
            "id": "73f924b2-7b1d-5fb8-826a-9a4cd71f8a49",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "hugging-face-endpoints"
            }
           },
           {
            "filename": "https://docs.pinecone.io/docs/hugging-face-endpoints",
            "id": "73f924b2-7b1d-5fb8-826a-9a4cd71f8a49",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "hugging-face-endpoints"
            }
           }
          ],
          [
           {
            "filename": "https://docs.pinecone.io/docs/insert-data",
            "id": "8199e897-1a6e-58be-8d2c-eed69c360275",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "insert-data"
            }
           }
          ],
          [
           {
            "filename": "https://docs.pinecone.io/docs/metadata-filtering",
            "id": "c4fc0451-bfca-5ab8-a2b4-19c5bafb59b3",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "metadata-filtering"
            }
           }
          ],
          [
           {
            "filename": "https://docs.pinecone.io/docs/quickstart",
            "id": "808d96b1-2045-508d-8c2b-6ad4610649bf",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "quickstart"
            }
           }
          ],
          [
           {
            "filename": "https://docs.pinecone.io/docs/manage-data",
            "id": "2335f977-beb6-5ea2-b524-ba0fb7fdbe3f",
            "metadata": {
             "created_at": "2023_10_25",
             "title": "manage-data"
            }
           }
          ]
         ],
         "question": [
          "What steps must be taken to shut down the inference endpoint in the context of the Pinecone vector database tutorial?",
          "What is the purpose of the upsert operation in the context of inserting records into a Pinecone index?",
          "What is the purpose of genre filtering in the context of querying data?",
          "What is the recommended method for upserting larger amounts of data in Pinecone?",
          "What are the two methods for updating records in Pinecone?"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "question",
           "type": "string"
          },
          {
           "name": "contexts",
           "type": "string"
          },
          {
           "name": "ground_truth",
           "type": "string"
          },
          {
           "name": "evolution_type",
           "type": "string"
          },
          {
           "name": "metadata",
           "type": "string"
          },
          {
           "name": "episode_done",
           "type": "boolean"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What steps must be taken to shut down the infe...</td>\n",
       "      <td>[s\": batch}\\n    )\\n    emb = res.json()['embe...</td>\n",
       "      <td>To shut down the inference endpoint, navigate ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'id': '73f924b2-7b1d-5fb8-826a-9a4cd71f8a49'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the upsert operation in...</td>\n",
       "      <td>[# Insert data\\n\\n[Suggest Edits](/edit/insert...</td>\n",
       "      <td>The purpose of the upsert operation in the con...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'id': '8199e897-1a6e-58be-8d2c-eed69c360275'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of genre filtering in the ...</td>\n",
       "      <td>[ 'namespace': ''}\\n\\n```\\n\\ncurl\\n```python\\n...</td>\n",
       "      <td>The purpose of genre filtering in the context ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'id': 'c4fc0451-bfca-5ab8-a2b4-19c5bafb59b3'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the recommended method for upserting l...</td>\n",
       "      <td>[4]\\n  },\\n  {\\n    \"id\": \"E\",\\n    \"values\": ...</td>\n",
       "      <td>The recommended method for upserting larger am...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'id': '808d96b1-2045-508d-8c2b-6ad4610649bf'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the two methods for updating records ...</td>\n",
       "      <td>[#              'id-2': {'id': 'id-2',\\n#     ...</td>\n",
       "      <td>The two methods for updating records in Pineco...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'id': '2335f977-beb6-5ea2-b524-ba0fb7fdbe3f'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  ... episode_done\n",
       "0  What steps must be taken to shut down the infe...  ...         True\n",
       "1  What is the purpose of the upsert operation in...  ...         True\n",
       "2  What is the purpose of genre filtering in the ...  ...         True\n",
       "3  What is the recommended method for upserting l...  ...         True\n",
       "4  What are the two methods for updating records ...  ...         True\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714745e0-b2eb-49f2-b947-46f67319c170",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 29998,
    "lastExecutedAt": 1722354401682,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def generate_test_results(testset):\n    results = []\n    for row in testset.itertuples():\n        query = row.question\n        contexts = retrieve_chunks(query)\n        context_str = \"\\n\".join(contexts)\n        answer = augmented_query(query, context_str)\n        results.append({\n            \"question\": query,\n            \"answer\": answer,\n            \"contexts\": contexts,\n            \"ground_truth\": row.ground_truth\n        })\n    return results\n\n# Generate results for the test set\ntest_results = generate_test_results(testset.to_pandas())"
   },
   "outputs": [],
   "source": [
    "def generate_test_results(testset):\n",
    "    results = []\n",
    "    for row in testset.itertuples():\n",
    "        query = row.question\n",
    "        contexts = retrieve_chunks(query)\n",
    "        context_str = \"\\n\".join(contexts)\n",
    "        answer = augmented_query(query, context_str)\n",
    "        results.append({\n",
    "            \"question\": query,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": contexts,\n",
    "            \"ground_truth\": row.ground_truth\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Generate results for the test set\n",
    "test_results = generate_test_results(testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766c7b84-ce16-40b7-9b73-b1132d30def9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 25,
    "lastExecutedAt": 1722354232844,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "pd.DataFrame(test_results, columns=['question', 'answer', 'contexts', 'ground_truth'])",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "answer": [
          "To shut down the inference endpoint in the context of the Pinecone vector database tutorial, you must navigate to the Inference Endpoints **Overview** page and select **Delete endpoint**.",
          "The purpose of the upsert operation in the context of inserting records into a Pinecone index is to write records into the index. The upsert operation allows you to add or update records in the index, ensuring that the records are inserted or updated as needed.",
          "The purpose of genre filtering in the context of querying data is to allow for the indexing of genres, facilitating specific searches or queries based on genre-related information.",
          "The recommended method for upserting larger amounts of data in Pinecone is to use a dataset with a large volume of documents to produce the embeddings and then upsert them into Pinecone.",
          "The two methods for updating records in Pinecone are using *full* updates or *partial* updates.",
          "To effectively query sparse-dense vectors, an index should have the capability to compute the full dot product over the entire vector. This means that the index should be able to calculate the dot product of the dense values in the vectors with the dense part of the query, as well as the dot product of the sparse values in the vectors with the sparse part of the query. Additionally, the index should allow for weighting the sparse or dense vector component more heavily to customize the relevance of the results.",
          "The portion of the index URL after the index name and before the dot is the project ID in Pinecone.",
          "To retrieve documents from a specific year in a movie index, you would need to apply the following filter:\n- The filter should be an `$and` operation with the conditions:\n  - \"genre\" is equal to \"drama\"\n  - \"year\" is greater than or equal to 2020\n\nTherefore, the filters needed for retrieving docs from a specific year in a movie index are `{ \"$and\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }] }`.",
          "To set up a secret in AWS Secrets Manager for a Pinecone index in Amazon Bedrock, follow these steps:\n\n1. Head to the Secrets Manager and create a new secret.\n2. Define your secret name.\n3. Provide a helpful description and click next.\n4. Select \"Other type of secret\".\n5. Create a new Key/value pair with the key `apiKey` and paste your Pinecone API key as its corresponding value.\n6. Click next to save your key.\n7. Select all the default options in the next screen.\n8. Copy your new secret’s ARN so that it’s available later.",
          "The operations that can modify records in the Pinecone API are insert, update, and upsert. \n\n- **Insert**: Adds a new record to the index.\n- **Update**: Modifies specific attributes of an existing record.\n- **Upsert**: Inserts a new record if it does not exist or updates an existing record.\n\nWhen using these operations to modify records in the Pinecone API, they can affect the whole record or specific attributes based on the operation used:\n\n- **Insert**: The insert operation affects the whole record by adding all attributes of the new record.\n- **Update**: The update operation affects specific attributes of the existing record while leaving the rest unchanged.\n- **Upsert**: The upsert operation can either modify specific attributes if the record already exists or add the entire record if it is new."
         ],
         "contexts": [
          [
           "All of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.\n\n\nAll of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.\nAll of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.\n\n**Once the index is deleted, you cannot use it again.**\n\n\n**Once the index is deleted, you cannot use it again.**\nOnce the index is deleted, you cannot use it again.\nOnce the index is deleted, you cannot use it again.\n\nShut down the endpoint by navigating to the Inference Endpoints **Overview** page and selecting **Delete endpoint**. Delete the Pinecone index with:\n\n\nShut down the endpoint by navigating to the Inference Endpoints \nShut down the endpoint by navigating to the Inference Endpoints **Overview**\nOverview\nOverview page and selecting \n page and",
           "# Quickstart\n\nQuickstart\nQuickstart\n\n[Suggest Edits](/edit/quickstart)This guide explains how to set up a Pinecone vector database in minutes.\n\n\n[Suggest Edits](/edit/quickstart)\nSuggest Edits\nSuggest EditsThis guide explains how to set up a Pinecone vector database in minutes.\nThis guide explains how to set up a Pinecone vector database in minutes.",
           "# Datadog\n\nDatadog\nDatadog\n\n[Suggest Edits](/edit/datadog)## Overview\n\n\n[Suggest Edits](/edit/datadog)\nSuggest Edits\nSuggest Edits## Overview\n## Overview\n\nThis topic describes how to use Datadog to monitor your Pinecone vector database.\n\n\nThis topic describes how to use Datadog to monitor your Pinecone vector database.\nThis topic describes how to use Datadog to monitor your Pinecone vector database."
          ],
          [
           "import { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: \"YOUR_API_KEY\",\n  environment: \"YOUR_ENVIRONMENT\"\n})\nconst index = pinecone.index(\"pinecone-index\")\n\n\nimport { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: \"YOUR_API_KEY\",\n  environment: \"YOUR_ENVIRONMENT\"\n})\nconst index = pinecone.index(\"pinecone-index\")\n\n\n\n```python\n# Not applicable\n\n```\n\n# Not applicable\n\n\n# Not applicable\n\n\n\n2. Use the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\nUse the upsert operation to write records into the index:\n\nPythonJavaScriptcurl\n\n\nPythonJavaScriptcurl\nPythonJavaScriptcurl",
           "Immediately after the upsert response is received, records may not be visible to queries yet. This is because Pinecone is eventually consistent. In most situations, you can check if the records have been received by checking for the record counts returned by `describe_index_stats()` to be updated. Keep in mind that if you have multiple replicas, they may not all become consistent at the same time.\n\n\nImmediately after the upsert response is received, records may not be visible to queries yet. This is because Pinecone is eventually consistent. In most situations, you can check if the records have been received by checking for the record counts returned by \nImmediately after the upsert response is received, records may not be visible to queries yet. This is because Pinecone is eventually consistent. In most situations, you can check if the records have been received by checking for the record counts returned by `describe_index_stats()`\ndescribe_index_stats()",
           "can query your index, you need to create a client instance that targets the index you just created.\n\nPythonJavaScriptcurl\n\n\nPythonJavaScriptcurl\nPythonJavaScriptcurl```python\nindex = pinecone.Index(\"quickstart\")\n\n```\n\nindex = pinecone.Index(\"quickstart\")\n\n\nindex = pinecone.Index(\"quickstart\")\n\n\n\n```python\nconst index = pinecone.index(\"quickstart\");\n\n```\n\nconst index = pinecone.index(\"quickstart\");\n\n\nconst index = pinecone.index(\"quickstart\");\n\n\n\n```python\n# Not applicable\n\n```\n\n# Not applicable\n\n\n# Not applicable\n\n\n\n5. Insert the data.\n\n\nInsert the data.\n\n\nInsert the data.\n\n\nInsert the data.\nInsert the data.\n\nTo ingest vectors into your index, use the [`upsert`](https://www.pinecone.io/docs/api/operation/upsert/) operation. \n\n\nTo ingest vectors into your index, use the \nTo ingest vectors into your index, use the [`upsert`](https://www.pinecone.io/docs/api/operation/upsert/)\n`upsert`\nupsert operation. \n operation."
          ],
          [
           "### Advantages\n\nAdvantages\nAdvantages\n\n* Metadata filtering allows you to query across multiple tenants.\n\n* Metadata filtering reduces the need for additional indexes, reducing maintenance effort.\n\n\nMetadata filtering allows you to query across multiple tenants.\n\n\nMetadata filtering allows you to query across multiple tenants.\n\n\nMetadata filtering allows you to query across multiple tenants.\nMetadata filtering allows you to query across multiple tenants.Metadata filtering reduces the need for additional indexes, reducing maintenance effort.\n\n\nMetadata filtering reduces the need for additional indexes, reducing maintenance effort.\n\n\nMetadata filtering reduces the need for additional indexes, reducing maintenance effort.\nMetadata filtering reduces the need for additional indexes, reducing maintenance effort.",
           "types of information and enable distinct kinds of search.",
           "\"indexed\": [\"genre\"]\n      }\n    }'"
          ],
          [
           "Pinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.\n\n\nPinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.\nPinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.",
           "Databricks and Pinecone are the perfect combination for working with very large vector datasets. Pinecone provides a way to efficiently store and retrieve the vectors created by Databricks, making it easy and performant to work with a huge number of vectors. Overall, the combination of Databricks and Pinecone provides a powerful and effective solution for creating embeddings for very large datasets. By parallelizing the embedding generation and the data ingestion processes, we can create a fast and resilient pipeline that will be able to index and update large volumes of vectors.",
           ". We'll then use a dataset with a large volume of documents to produce the embeddings and upsert them into Pinecone. Note that the actual model and dataset we'll use are immaterial for this example. This method should work on any embeddings you may want to create, with whatever dataset you may choose.\n. We'll then use a dataset with a large volume of documents to produce the embeddings and upsert them into Pinecone. Note that the actual model and dataset we'll use are immaterial for this example. This method should work on any embeddings you may want to create, with whatever dataset you may choose.\n\nIn order to create embeddings at scale, we need to do four things:\n\n\nIn order to create embeddings at scale, we need to do four things:\nIn order to create embeddings at scale, we need to do four things:\n\n1. Set up a Spark cluster\n\n2. Load the dataset into partitions\n\n3. Apply an embedding model on each entry to produce the embedding\n\n4. Save the results\n\n\nSet up a Spark cluster"
          ],
          [
           "Access the [Pinecone Console](https://app.pinecone.io).\n\n\nAccess the \nAccess the [Pinecone Console](https://app.pinecone.io)\nPinecone Console\nPinecone Console.\n.Click **Settings** in the left menu.\n\n\nClick **Settings** in the left menu.\n\n\nClick \nClick **Settings**\nSettings\nSettings in the left menu.\n in the left menu.In the **Settings** view, click the **PROJECTS** tab.\n\n\nIn the **Settings** view, click the **PROJECTS** tab.\n\n\nIn the \nIn the **Settings**\nSettings\nSettings view, click the \n view, click the **PROJECTS**\nPROJECTS\nPROJECTS tab.\n tab.Next to the project you want to update, click ![edit icon](https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png).\n\n\nNext to the project you want to update, click ![edit icon](https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png).\n\n\nNext to the project you want to update, click \nNext to the project you want to update, click ![edit icon](https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png)",
           "## Updating records\n\nUpdating records\nUpdating records\n\nThere are two methods for updating records and metadata, using *full* or *partial* updates.\n\n\nThere are two methods for updating records and metadata, using \nThere are two methods for updating records and metadata, using *full*\nfull\nfull or \n or *partial*\npartial\npartial updates.\n updates.",
           "Pinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.\n\n\nPinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.\nPinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size."
          ],
          [
           "## Querying sparse-dense vectors\n\nQuerying sparse-dense vectors\nQuerying sparse-dense vectors\n\nTo query your sparse-dense vectors, you provide a query vector containing both sparse and dense values. Pinecone ranks vectors in your index by considering the full dot product over the entire vector; the score of a vector is the sum of the dot product of its dense values with the dense part of the query, together with the dot product of its sparse values with the sparse part of the query. You may want to [weight the sparse or dense vector component more heavily](weighting-sparse-and-dense-vectors).",
           "ine the results for more relevant results. This topic describes how to weight sparse versus dense vectors when querying your index.\n\nTo see sparse-dense embeddings in action, see the [Ecommerce hybrid search example](ecommerce-search).\n\n\nTo see sparse-dense embeddings in action, see the \nTo see sparse-dense embeddings in action, see the [Ecommerce hybrid search example](ecommerce-search)\nEcommerce hybrid search example\nEcommerce hybrid search example.\n.",
           "To query your sparse-dense vectors, you provide a query vector containing both sparse and dense values. Pinecone ranks vectors in your index by considering the full dot product over the entire vector; the score of a vector is the sum of the dot product of its dense values with the dense part of the query, together with the dot product of its sparse values with the sparse part of the query. You may want to \nTo query your sparse-dense vectors, you provide a query vector containing both sparse and dense values. Pinecone ranks vectors in your index by considering the full dot product over the entire vector; the score of a vector is the sum of the dot product of its dense values with the dense part of the query, together with the dot product of its sparse values with the sparse part of the query. You may want to [weight the sparse or dense vector component more heavily](weighting-sparse-and-dense-vectors)\nweight the sparse or dense vector component more heavily\nweight the sparse or dense ve"
          ],
          [
           "## Project ID\n\nProject ID\nProject ID\n\nEach Pinecone project has a project ID. This hexadecimal string appears as part of the URL for API calls. \n\n\nEach Pinecone project has a project ID. This hexadecimal string appears as part of the URL for API calls. \nEach Pinecone project has a project ID. This hexadecimal string appears as part of the URL for API calls. \n\nTo find a project's ID, follow these steps:\n\n\nTo find a project's ID, follow these steps:\nTo find a project's ID, follow these steps:\n\n1. Go to the [Pinecone console](https://app.pinecone.io).\n\n2. In the upper-left corner, select your project.\n\n3. Click **Indexes**.\n\n4. Under the name of your indexes, find the index URL. For example:\n\n\nGo to the [Pinecone console](https://app.pinecone.io).\n\n\nGo to the [Pinecone console](https://app.pinecone.io).\n\n\nGo to the \nGo to the [Pinecone console](https://app.pinecone.io)\nPinecone console\nPinecone console.\n.In the upper-left corner, select your project.",
           "In the upper-left corner, select your project.\n\n\nIn the upper-left corner, select your project.\nIn the upper-left corner, select your project.Click **Indexes**.\n\n\nClick **Indexes**.\n\n\nClick \nClick **Indexes**\nIndexes\nIndexes.\n.Under the name of your indexes, find the index URL. For example:\n\n\nUnder the name of your indexes, find the index URL. For example:\n\n\nUnder the name of your indexes, find the index URL. For example:\nUnder the name of your indexes, find the index URL. For example:\n\n`example-index-1e3g52e.svc.us-east1-gcp.pinecone.io`\n\n\n`example-index-1e3g52e.svc.us-east1-gcp.pinecone.io`\nexample-index-1e3g52e.svc.us-east1-gcp.pinecone.io\n\nThe portion of the index URL after the index name and before the dot is the project ID. \n\n\nThe portion of the index URL after the index name and before the dot is the project ID. \nThe portion of the index URL after the index name and before the dot is the project ID.",
           "st project in Pinecone."
          ],
          [
           "This example deletes all vectors with genre \"documentary\" and year 2019 from an index.\n\n\nThis example deletes all vectors with genre \"documentary\" and year 2019 from an index.\nThis example deletes all vectors with genre \"documentary\" and year 2019 from an index.\n\nPythonJavaScriptcurl\n\n\nPythonJavaScriptcurl\nPythonJavaScriptcurl```python\nindex.delete(\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    }\n)\n\n```\n\nindex.delete(\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    }\n)\n\n\nindex.delete(\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    }\n)\n\n\n\n```python\nawait index._delete({\n  deleteRequest: {\n    filter: {\n    genre: { $eq: \"documentary\" },\n    year: 2019,\n  }\n});\n\n```\n\nawait index._delete({\n  deleteRequest: {\n    filter: {\n    genre: { $eq: \"documentary\" },\n    year: 2019,\n  }\n});\n\n\nawait index._delete({\n  deleteRequest: {\n    filter: {\n    genre: { $eq: \"documentary\" },\n    year: 2019,\n  }\n});",
           "## Querying an index with metadata filters\n\nQuerying an index with metadata filters\nQuerying an index with metadata filters\n\nMetadata filter expressions can be included with queries to limit the search to only vectors matching the filter expression.\n\n\nMetadata filter expressions can be included with queries to limit the search to only vectors matching the filter expression.\nMetadata filter expressions can be included with queries to limit the search to only vectors matching the filter expression.\n\nFor example, we can search the previous movies index for documentaries from the year 2019. This also uses the `include_metadata` flag so that vector metadata is included in the response.\n\n\nFor example, we can search the previous movies index for documentaries from the year 2019. This also uses the \nFor example, we can search the previous movies index for documentaries from the year 2019. This also uses the `include_metadata`\ninclude_metadata",
           "{\n  \"$and\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n\n{\n  \"$and\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n\n\nA drama or a movie from 2020:\n\n\nA drama or a movie from 2020:\nA drama or a movie from 2020:\n\nJSON\n\n\nJSON\nJSON```python\n{\n  \"$or\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n```\n\n{\n  \"$or\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}\n\n\n{\n  \"$or\": [{ \"genre\": { \"$eq\": \"drama\" } }, { \"year\": { \"$gte\": 2020 } }]\n}"
          ],
          [
           "### Setting up secrets\n\nSetting up secrets\nSetting up secrets\n\nAfter setting up your Pinecone index, you’ll have to create a secret in AWS’s Secrets Manager. \n\n\nAfter setting up your Pinecone index, you’ll have to create a secret in AWS’s Secrets Manager. \nAfter setting up your Pinecone index, you’ll have to create a secret in AWS’s Secrets Manager. \n\n1. Head to the Secrets Manager and create a new secret.\n\n2. Define your secret name.\n\n3. Provide a helpful description and click next.\n\n\nHead to the Secrets Manager and create a new secret.\n\n\nHead to the Secrets Manager and create a new secret.\n\n\nHead to the Secrets Manager and create a new secret.\nHead to the Secrets Manager and create a new secret.Define your secret name.\n\n\nDefine your secret name.\n\n\nDefine your secret name.\nDefine your secret name.Provide a helpful description and click next.\n\n\nProvide a helpful description and click next.\n\n\nProvide a helpful description and click next.\nProvide a helpful description and click next.",
           "![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen2.png)\n4. Select “Other type of secret”.\n5. Create a new Key/value pair with the key `apiKey` and then paste your Pinecone API key as it’s corresponding value.\n6. Click next to save your key.\n\n\n![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen2.png)\n\n\n\n4. Select “Other type of secret”.\n4. Select “Other type of secret”.\n\n\n5. Create a new Key/value pair with the key \n5. Create a new Key/value pair with the key `apiKey`\napiKey and then paste your Pinecone API key as it’s corresponding value.\n and then paste your Pinecone API key as it’s corresponding value.\n\n\n6. Click next to save your key.\n6. Click next to save your key.\n\n![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen1.png)\n7. Select all the default options in the next screen.",
           "![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen1.png)\n\n\n\n7. Select all the default options in the next screen.\n7. Select all the default options in the next screen.\n\n![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen3.png)\n8. Copy your new secret’s ARN so that it’s available later.\n\n\n![](https://raw.githubusercontent.com/pinecone-io/img/main/amazon-bedrock/CreateSecret-Screen3.png)\n\n\n\n8. Copy your new secret’s ARN so that it’s available later.\n8. Copy your new secret’s ARN so that it’s available later.\n\n![]()\n\n\n![]()"
          ],
          [
           "import { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: \"YOUR_API_KEY\",\n  environment: \"YOUR_ENVIRONMENT\"\n})\nconst index = pinecone.index(\"pinecone-index\")\n\n\nimport { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: \"YOUR_API_KEY\",\n  environment: \"YOUR_ENVIRONMENT\"\n})\nconst index = pinecone.index(\"pinecone-index\")\n\n\n\n```python\n# Not applicable\n\n```\n\n# Not applicable\n\n\n# Not applicable\n\n\n\n2. Use the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\n\n\nUse the upsert operation to write records into the index:\nUse the upsert operation to write records into the index:\n\nPythonJavaScriptcurl\n\n\nPythonJavaScriptcurl\nPythonJavaScriptcurl",
           "f pods in use, pod size, the total time each pod is running, and the billing plan. This topic describes several ways you can manage your overall Pinecone cost by adjusting these variables.",
           "# Manage data\n\nManage data\nManage data\n\n[Suggest Edits](/edit/manage-data)In addition to [inserting](insert-data) and [querying](query-data) data, there are other ways you can interact with vector data in a Pinecone index. This section walks through the various vector operations available.\n\n\n[Suggest Edits](/edit/manage-data)\nSuggest Edits\nSuggest EditsIn addition to \nIn addition to [inserting](insert-data)\ninserting\ninserting and \n and [querying](query-data)\nquerying\nquerying data, there are other ways you can interact with vector data in a Pinecone index. This section walks through the various vector operations available.\n data, there are other ways you can interact with vector data in a Pinecone index. This section walks through the various vector operations available."
          ]
         ],
         "ground_truth": [
          "To shut down the inference endpoint, navigate to the Inference Endpoints Overview page and select Delete endpoint. Additionally, delete the Pinecone index using the command: pinecone.delete_index(index_name).",
          "The purpose of the upsert operation in the context of inserting records into a Pinecone index is to write records into the index, allowing for the insertion of vector embeddings and metadata.",
          "The purpose of genre filtering in the context of querying data is to specify which genres of data (such as comedy, documentary, or drama) should be included in the query results. This allows users to narrow down their search to specific types of content based on their preferences.",
          "The recommended method for upserting larger amounts of data in Pinecone is to upsert data in batches of 100 vectors or fewer over multiple upsert requests.",
          "The two methods for updating records in Pinecone are full updates and partial updates.",
          "For effective querying of sparse-dense vectors, an index must use the `dotproduct` metric. Attempting to query any other index with sparse values will return an error.",
          "The project ID in Pinecone is shown in the index URL, specifically the portion of the URL after the index name and before the dot.",
          "To retrieve documents from a specific year in a movie index, you can use a metadata filter expression that specifies the year. For example, to filter for movies from the year 2019, you would use: {\"year\": {\"$eq\": 2019}}.",
          "To set up a secret in AWS Secrets Manager for a Pinecone index in Amazon Bedrock, follow these steps: 1. Head to the Secrets Manager and create a new secret. 2. Define your secret name. 3. Provide a helpful description and click next. 4. Select 'Other type of secret'. 5. Create a new Key/value pair with the key `apiKey` and paste your Pinecone API key as its corresponding value. 6. Click next to save your key. 7. Select all the default options in the next screen. 8. Copy your new secret’s ARN for later use.",
          "The operations that can modify records in the Pinecone API include full updates and partial updates. Full updates modify the entire record, including vector values and metadata, while partial updates allow for modifications to specific attributes without changing the entire record."
         ],
         "index": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "question": [
          "What steps must be taken to shut down the inference endpoint in the context of the Pinecone vector database tutorial?",
          "What is the purpose of the upsert operation in the context of inserting records into a Pinecone index?",
          "What is the purpose of genre filtering in the context of querying data?",
          "What is the recommended method for upserting larger amounts of data in Pinecone?",
          "What are the two methods for updating records in Pinecone?",
          "What should an index have for effective querying of sparse-dense vectors?",
          "Which URL part shows the project ID in Pinecone?",
          "What filters are needed for retrieving docs from a specific year in a movie index?",
          "What steps to set up a secret in AWS Secrets Manager for a Pinecone index in Amazon Bedrock?",
          "What ops can modify records in Pinecone API, and how do they affect the whole record vs. specific attributes?"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "question",
           "type": "string"
          },
          {
           "name": "answer",
           "type": "string"
          },
          {
           "name": "contexts",
           "type": "string"
          },
          {
           "name": "ground_truth",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 10,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What steps must be taken to shut down the infe...</td>\n",
       "      <td>To shut down the inference endpoint in the con...</td>\n",
       "      <td>[All of these results look excellent. If you a...</td>\n",
       "      <td>To shut down the inference endpoint, navigate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the upsert operation in...</td>\n",
       "      <td>The purpose of the upsert operation in the con...</td>\n",
       "      <td>[import { Pinecone } from '@pinecone-database/...</td>\n",
       "      <td>The purpose of the upsert operation in the con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of genre filtering in the ...</td>\n",
       "      <td>The purpose of genre filtering in the context ...</td>\n",
       "      <td>[### Advantages\\n\\nAdvantages\\nAdvantages\\n\\n*...</td>\n",
       "      <td>The purpose of genre filtering in the context ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the recommended method for upserting l...</td>\n",
       "      <td>The recommended method for upserting larger am...</td>\n",
       "      <td>[Pinecone lets us efficiently ingest, update a...</td>\n",
       "      <td>The recommended method for upserting larger am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the two methods for updating records ...</td>\n",
       "      <td>The two methods for updating records in Pineco...</td>\n",
       "      <td>[Access the [Pinecone Console](https://app.pin...</td>\n",
       "      <td>The two methods for updating records in Pineco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What should an index have for effective queryi...</td>\n",
       "      <td>To effectively query sparse-dense vectors, an ...</td>\n",
       "      <td>[## Querying sparse-dense vectors\\n\\nQuerying ...</td>\n",
       "      <td>For effective querying of sparse-dense vectors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which URL part shows the project ID in Pinecone?</td>\n",
       "      <td>The portion of the index URL after the index n...</td>\n",
       "      <td>[## Project ID\\n\\nProject ID\\nProject ID\\n\\nEa...</td>\n",
       "      <td>The project ID in Pinecone is shown in the ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What filters are needed for retrieving docs fr...</td>\n",
       "      <td>To retrieve documents from a specific year in ...</td>\n",
       "      <td>[This example deletes all vectors with genre \"...</td>\n",
       "      <td>To retrieve documents from a specific year in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What steps to set up a secret in AWS Secrets M...</td>\n",
       "      <td>To set up a secret in AWS Secrets Manager for ...</td>\n",
       "      <td>[### Setting up secrets\\n\\nSetting up secrets\\...</td>\n",
       "      <td>To set up a secret in AWS Secrets Manager for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What ops can modify records in Pinecone API, a...</td>\n",
       "      <td>The operations that can modify records in the ...</td>\n",
       "      <td>[import { Pinecone } from '@pinecone-database/...</td>\n",
       "      <td>The operations that can modify records in the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  ...                                       ground_truth\n",
       "0  What steps must be taken to shut down the infe...  ...  To shut down the inference endpoint, navigate ...\n",
       "1  What is the purpose of the upsert operation in...  ...  The purpose of the upsert operation in the con...\n",
       "2  What is the purpose of genre filtering in the ...  ...  The purpose of genre filtering in the context ...\n",
       "3  What is the recommended method for upserting l...  ...  The recommended method for upserting larger am...\n",
       "4  What are the two methods for updating records ...  ...  The two methods for updating records in Pineco...\n",
       "5  What should an index have for effective queryi...  ...  For effective querying of sparse-dense vectors...\n",
       "6   Which URL part shows the project ID in Pinecone?  ...  The project ID in Pinecone is shown in the ind...\n",
       "7  What filters are needed for retrieving docs fr...  ...  To retrieve documents from a specific year in ...\n",
       "8  What steps to set up a secret in AWS Secrets M...  ...  To set up a secret in AWS Secrets Manager for ...\n",
       "9  What ops can modify records in Pinecone API, a...  ...  The operations that can modify records in the ...\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results, columns=['question', 'answer', 'contexts', 'ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52a75fdc-6fb8-4f17-9135-2cd5b88c43bf",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 57206,
    "lastExecutedAt": 1722354638850,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "\n# Convert results to a Hugging Face Dataset\nfrom datasets import Dataset\nevaluation_dataset = Dataset.from_list(test_results)\n\n# Now you can use this dataset for evaluation\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    context_entity_recall,\n    answer_similarity,\n    answer_correctness    \n)\n\nresult = evaluate(\n    evaluation_dataset,\n    metrics=[\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall,        \n        context_entity_recall,\n        answer_similarity,\n        answer_correctness\n    ],\n)\n\n"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6816db32ae304bdfa3f1df5435c7b0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Convert results to a Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "evaluation_dataset = Dataset.from_list(test_results)\n",
    "\n",
    "# Now you can use this dataset for evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness    \n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,        \n",
    "        context_entity_recall,\n",
    "        answer_similarity,\n",
    "        answer_correctness\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "426bca77-b761-4e89-ad0c-19e642669c98",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 168,
    "lastExecutedAt": 1722354639020,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "result_df = pd.DataFrame(result.items(), columns=['Metric', 'Value'])\nresult_df",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "Metric": [
          "context_precision",
          "faithfulness",
          "answer_relevancy",
          "context_recall",
          "context_entity_recall",
          "answer_similarity",
          "answer_correctness"
         ],
         "Value": [
          0.9,
          0.7033333333,
          0.9521695956,
          0.75,
          0.3649999976,
          0.9416385257,
          0.5904363231
         ],
         "index": [
          0,
          1,
          2,
          3,
          4,
          5,
          6
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "Metric",
           "type": "string"
          },
          {
           "name": "Value",
           "type": "number"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 7,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.952170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>context_entity_recall</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>answer_similarity</td>\n",
       "      <td>0.941639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.590436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Metric     Value\n",
       "0      context_precision  0.900000\n",
       "1           faithfulness  0.703333\n",
       "2       answer_relevancy  0.952170\n",
       "3         context_recall  0.750000\n",
       "4  context_entity_recall  0.365000\n",
       "5      answer_similarity  0.941639\n",
       "6     answer_correctness  0.590436"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result.items(), columns=['Metric', 'Value'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5c8d29b-caf5-415a-b15e-039d47e54bdb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "lastExecutedAt": 1722354317022,
    "lastExecutedByKernel": "353ff92d-28cb-4608-8730-a603ed2b2978",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "result_df = pd.DataFrame(result.items(), columns=['Metric', 'Value'])\nresult_df",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "5b68bab8-6cf6-4e54-83fe-811ee9ec4dae",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "Metric": [
          "context_precision",
          "faithfulness",
          "answer_relevancy",
          "context_recall",
          "context_entity_recall",
          "answer_similarity",
          "answer_correctness"
         ],
         "Value": [
          0.9416666666,
          0.7071428571,
          0.9731178434,
          0.65,
          0.302499999,
          0.9529949663,
          0.5637945163
         ],
         "index": [
          0,
          1,
          2,
          3,
          4,
          5,
          6
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "Metric",
           "type": "string"
          },
          {
           "name": "Value",
           "type": "number"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 7,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.707143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.973118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>context_entity_recall</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>answer_similarity</td>\n",
       "      <td>0.952995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.563795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Metric     Value\n",
       "0      context_precision  0.941667\n",
       "1           faithfulness  0.707143\n",
       "2       answer_relevancy  0.973118\n",
       "3         context_recall  0.650000\n",
       "4  context_entity_recall  0.302500\n",
       "5      answer_similarity  0.952995\n",
       "6     answer_correctness  0.563795"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result.items(), columns=['Metric', 'Value'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5503c5d-45b0-410b-b564-674a3226434c",
   "metadata": {},
   "source": [
    "# Pinecone Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a90ec-0379-422f-b3c2-98d729d53b18",
   "metadata": {},
   "source": [
    "## Enterprise RAG - Easy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324fbe4-02c2-476b-bcc4-0e874c3f318b",
   "metadata": {},
   "source": [
    "* Simplicity: You can easily add their files and start building AI applications using the API. \n",
    "* High-quality results: It provides relevant answers grounded in the user's data, along with references. \n",
    "* Comprehensive infrastructure: It handles all system operations, including chunking, embedding, file storage, query planning, vector search, model orchestration, and reranking. \n",
    "* Accuracy and reliability: Pinecone Assistant focuses on delivering high-quality and dependable answers \n",
    "* Enterprise-grade security: Even in beta, it's powered by components that meet strict compliance requirements. \n",
    "* Data protection and control: User data is encrypted, isolated, and used only as context for answers in real-time, without permanently fine-tuning or training the underlying language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac28ef4c-55d6-4879-ad77-aa6d7a338706",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 2989,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1722026634457,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install --upgrade \"pinecone-client[grpc]\" pinecone-plugin-assistant",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade \"pinecone-client[grpc]\" pinecone-plugin-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "115d29e5-7092-476f-b334-30062ff3699d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5470,
    "lastExecutedAt": 1722026711922,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "metadata = {\"author\": \"Roie Schwaber-Cohen\", \"version\": \"1.0\"}\n\nassistant = pinecone_client.assistant.create_assistant(\n    assistant_name=\"datacamp-demo\", \n    metadata=metadata, \n    timeout=30 # Wait 30 seconds for assistant operation to complete.\n)"
   },
   "outputs": [],
   "source": [
    "metadata = {\"author\": \"Roie Schwaber-Cohen\", \"version\": \"1.0\"}\n",
    "\n",
    "assistant = pinecone_client.assistant.create_assistant(\n",
    "    assistant_name=\"datacamp-demo\", \n",
    "    metadata=metadata, \n",
    "    timeout=30 # Wait 30 seconds for assistant operation to complete.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637f5e0-8d43-44fd-8c44-2fa2aa0bde90",
   "metadata": {},
   "source": [
    "List assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5982d11-4bb2-43ce-a2eb-c82b8fbe8f59",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 153,
    "lastExecutedAt": 1722026762504,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "assistants = pinecone_client.assistant.list_assistants()\nassistants"
   },
   "outputs": [],
   "source": [
    "assistants = pinecone_client.assistant.list_assistants()\n",
    "assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a55e3f24-76cc-4850-9d65-26670871516f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 745,
    "lastExecutedAt": 1722027371629,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\n\n# Create a directory to save the text files if it doesn't exist\noutput_dir = \"text_files\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the first five rows of the \"text\" field into txt files\nfor idx, row in data.iterrows():\n    file_path = os.path.join(output_dir, f\"text_{idx}.txt\")\n    with open(file_path, \"w\") as file:\n        file.write(row[\"text\"])\n\n# Return the path where the files are saved\noutput_dir"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to save the text files if it doesn't exist\n",
    "output_dir = \"text_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the first five rows of the \"text\" field into txt files\n",
    "for idx, row in data.iterrows():\n",
    "    file_path = os.path.join(output_dir, f\"text_{idx}.txt\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(row[\"text\"])\n",
    "\n",
    "# Return the path where the files are saved\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d3557fe-482f-4ffb-868f-aa36c30ab8c0",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 13,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1722027375644,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\n\n# List the contents of the text_files directory\noutput_dir = \"text_files\"\nfiles = os.listdir(output_dir)\nfiles"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List the contents of the text_files directory\n",
    "output_dir = \"text_files\"\n",
    "files = os.listdir(output_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8739d6d9-ef2a-4460-985c-1ba4e627da8f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 949885,
    "lastExecutedAt": 1722028334835,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Iterate over each file in the text_files directory and upload it using the assistant\nfor file_name in files:\n    file_path = os.path.join(output_dir, file_name)\n    response = assistant.upload_file(\n        file_path=file_path,\n        timeout=None\n    )"
   },
   "outputs": [],
   "source": [
    "# Iterate over each file in the text_files directory and upload it using the assistant\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    response = assistant.upload_file(\n",
    "        file_path=file_path,\n",
    "        timeout=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ad691-c770-42d9-9a7d-b389ab727749",
   "metadata": {},
   "source": [
    "## Chat with the Assistnat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01dfc230-7c41-45ba-87b5-19466b40731c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1722027233892,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from pinecone_plugins.assistant.models.chat import Message"
   },
   "outputs": [],
   "source": [
    "from pinecone_plugins.assistant.models.chat import Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1158faf9-1d47-4ea7-a9e8-4129545b77e9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12314,
    "lastExecutedAt": 1722030953679,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "chat_context = [Message(content='How do I create an index in Python?')]\nresponse = assistant.chat_completions(messages=chat_context)"
   },
   "outputs": [],
   "source": [
    "chat_context = [Message(content='How do I create an index in Python?')]\n",
    "response = assistant.chat_completions(messages=chat_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1fd24ae-e88c-4b99-9c4b-c7497175f667",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "lastExecutedAt": 1722030969846,
    "lastExecutedByKernel": "fe7f35cd-4494-4592-8d8a-b28489539f4a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "display(Markdown(response.choices[0].message.content))"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
